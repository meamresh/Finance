{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from newspaper import Article, Config\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config set. Query: Bitcoin\n"
     ]
    }
   ],
   "source": [
    "QUERY = \"Bitcoin\"\n",
    "YEARS_BACK = 2\n",
    "GDELT_BASE = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "OUT_PARQUET = f\"gdelt_{QUERY}_2yrs_finbert.parquet\"\n",
    "OUT_WEEKLY_PNG = f\"gdelt_{QUERY}_2yrs_weekly_sentiment.png\"\n",
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "MAX_PER_CALL = 250\n",
    "PAUSE_BETWEEN_CALLS = 5  # FIXED: GDELT requires 5 seconds minimum\n",
    "PAUSE_BETWEEN_DOWNLOADS = 0.5\n",
    "MAX_ARTICLE_CHUNKS = 6\n",
    "CHUNK_CHARS = 1000\n",
    "END_DATE = datetime.utcnow()\n",
    "START_DATE = END_DATE - timedelta(days=365 * YEARS_BACK)\n",
    "TEMP_META = 'gdelt_query_urls.jsonl'\n",
    "RANDOM_STATE = 42\n",
    "ARTICLES_PER_WEEK = 10\n",
    "\n",
    "print('Config set. Query:', QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for GDELT querying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdelt_datefmt(dt: datetime) -> str:\n",
    "    \"\"\"Convert datetime to GDELT format.\"\"\"\n",
    "    return dt.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "\n",
    "def query_gdelt(query: str, start_dt: datetime, end_dt: datetime, maxrecords: int = MAX_PER_CALL):\n",
    "    \"\"\"Query GDELT API for articles.\"\"\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'mode': 'artlist',\n",
    "        'maxrecords': maxrecords,\n",
    "        'format': 'json',\n",
    "        'startdatetime': gdelt_datefmt(start_dt),\n",
    "        'enddatetime': gdelt_datefmt(end_dt),\n",
    "        'sort': 'datedesc'\n",
    "    }\n",
    "    try:\n",
    "        resp = requests.get(GDELT_BASE, params=params, timeout=60)\n",
    "        if resp.status_code != 200:\n",
    "            logging.warning('GDELT query failed: %s -> %s', resp.status_code, resp.text[:200])\n",
    "            return None\n",
    "        return resp.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f'GDELT request exception: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def query_gdelt_with_retry(query: str, start_dt: datetime, end_dt: datetime,\n",
    "                           maxrecords: int = MAX_PER_CALL, max_retries: int = 3):\n",
    "    \"\"\"Query GDELT with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        result = query_gdelt(query, start_dt, end_dt, maxrecords)\n",
    "        if result is not None:\n",
    "            return result\n",
    "        if attempt < max_retries - 1:\n",
    "            wait_time = (attempt + 1) * 5\n",
    "            logging.warning(f'Retrying GDELT query in {wait_time}s (attempt {attempt+1}/{max_retries})')\n",
    "            time.sleep(wait_time)\n",
    "    logging.error(f'GDELT query failed after {max_retries} attempts')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to iterate month windows\n",
    "\n",
    "def iterate_month_windows(start: datetime, end: datetime):\n",
    "    \"\"\"Generate month-by-month windows between start and end dates.\"\"\"\n",
    "    cur = datetime(start.year, start.month, 1)\n",
    "    while cur < end:\n",
    "        if cur.month == 12:\n",
    "            nxt = datetime(cur.year + 1, 1, 1)\n",
    "        else:\n",
    "            nxt = datetime(cur.year, cur.month + 1, 1)\n",
    "        yield cur, min(nxt - timedelta(seconds=1), end)\n",
    "        cur = nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to fetch all GDELT URLs with resume capability \n",
    "\n",
    "def fetch_all_gdelt_urls(query: str, start: datetime, end: datetime, out_meta_file: str = TEMP_META):\n",
    "    \"\"\"Query GDELT month-by-month and write metadata as JSONL for resume capability.\"\"\"\n",
    "    seen_urls = set()\n",
    "\n",
    "    # Resume if meta file exists\n",
    "    if os.path.exists(out_meta_file):\n",
    "        logging.info(\"Resuming: loading existing metadata to avoid re-fetching\")\n",
    "        with open(out_meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    o = json.loads(line)\n",
    "                    seen_urls.add(o.get(\"url\"))\n",
    "                except (json.JSONDecodeError, KeyError) as e:\n",
    "                    logging.debug(f\"Skipping malformed line: {e}\")\n",
    "                    continue\n",
    "\n",
    "    with open(out_meta_file, \"a\", encoding=\"utf-8\") as out:\n",
    "        for wstart, wend in iterate_month_windows(start, end):\n",
    "            logging.info(\"Querying GDELT for %s -> %s\", wstart.date(), wend.date())\n",
    "            resp = query_gdelt_with_retry(query, wstart, wend)\n",
    "            time.sleep(PAUSE_BETWEEN_CALLS)\n",
    "\n",
    "            if not resp:\n",
    "                continue\n",
    "\n",
    "            articles = resp.get(\"articles\") or resp.get(\"artlist\") or []\n",
    "            logging.info(\"Found %d articles in this window\", len(articles))\n",
    "\n",
    "            for a in articles:\n",
    "                url = a.get(\"url\") or a.get(\"sourceurl\") or a.get(\"documentidentifier\")\n",
    "                title = a.get(\"title\") or a.get(\"urltitle\") or \"\"\n",
    "                seendate = a.get(\"seendate\") or a.get(\"date\") or a.get(\"published\")\n",
    "\n",
    "                if not url or url in seen_urls:\n",
    "                    continue\n",
    "\n",
    "                seen_urls.add(url)\n",
    "                record = {\"url\": url, \"title\": title, \"seendate\": seendate}\n",
    "                out.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    logging.info(\"GDELT URL fetching complete. Total unique URLs: %d\", len(seen_urls))\n",
    "    return out_meta_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load URLs from metadata file\n",
    "\n",
    "def load_urls_from_meta(meta_file: str):\n",
    "    \"\"\"Load article metadata from JSONL file.\"\"\"\n",
    "    rows = []\n",
    "    with open(meta_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                logging.debug(f\"Skipping invalid JSON line: {e}\")\n",
    "                continue\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text using newspaper3k \n",
    "\n",
    "def extract_text_with_newspaper(url: str, fallback_text: str = \"\"):\n",
    "    \"\"\"Extract article text using newspaper3k with proper timeout configuration.\"\"\"\n",
    "    try:\n",
    "        config = Config()\n",
    "        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        config.request_timeout = 15\n",
    "        config.number_threads = 1\n",
    "\n",
    "        art = Article(url, config=config, language='en')\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        return art.title or None, art.text or None, (art.publish_date if hasattr(art, \"publish_date\") else None)\n",
    "    except Exception as e:\n",
    "        logging.debug(f\"Article extraction failed for {url}: {type(e).__name__}\")\n",
    "        return None, fallback_text or None, None\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_chars=CHUNK_CHARS):\n",
    "    \"\"\"Split text into chunks based on character limit.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    text = text.strip()\n",
    "    if len(text) <= max_chars:\n",
    "        return [text]\n",
    "\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    cur, cur_len = [], 0\n",
    "\n",
    "    for s in sentences:\n",
    "        s_len = len(s) + 2\n",
    "        if cur_len + s_len > max_chars and cur:\n",
    "            chunks.append('. '.join(cur).strip() + ('.' if not cur[-1].endswith('.') else ''))\n",
    "            cur = [s]\n",
    "            cur_len = s_len\n",
    "        else:\n",
    "            cur.append(s)\n",
    "            cur_len += s_len\n",
    "\n",
    "    if cur:\n",
    "        chunks.append('. '.join(cur).strip() + ('.' if not cur[-1].endswith('.') else ''))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to safely save DataFrame to Parquet with error handling \n",
    "def safe_to_parquet(df, path, **kwargs):\n",
    "    \"\"\"Safely save to parquet with ArrowKeyError handling.\"\"\"\n",
    "    import warnings\n",
    "    try:\n",
    "        df.to_parquet(path, engine='pyarrow', **kwargs)\n",
    "    except Exception as e:\n",
    "        if \"pandas.period\" in str(e) or \"ArrowKeyError\" in str(e):\n",
    "            logging.warning(\"Using fastparquet due to period type conflict\")\n",
    "            try:\n",
    "                df.to_parquet(path, engine='fastparquet', **kwargs)\n",
    "            except ImportError:\n",
    "                logging.error(\"fastparquet not installed. Run: pip install fastparquet\")\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def sanitize_df_for_parquet(df, convert_period_to='timestamp'):\n",
    "    \"\"\"Enhanced version with warning suppression and Period detection.\"\"\"\n",
    "    import warnings\n",
    "    df = df.copy()\n",
    "\n",
    "    # Handle PeriodIndex\n",
    "    if isinstance(df.index, pd.PeriodIndex):\n",
    "        df.index = df.index.to_timestamp() if convert_period_to == 'timestamp' else df.index.astype(str)\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_dtype = df[col].dtype\n",
    "        col_lower = col.lower()\n",
    "\n",
    "        # Date/time columns with warning suppression\n",
    "        if any(token in col_lower for token in (\"date\", \"time\", \"published\")):\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "                    parsed = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "                if parsed.notna().any():\n",
    "                    df[col] = parsed\n",
    "                else:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                continue\n",
    "            except:\n",
    "                df[col] = df[col].astype(str)\n",
    "                continue\n",
    "\n",
    "        # Handle PeriodDtype columns\n",
    "        try:\n",
    "            from pandas import PeriodDtype\n",
    "            if isinstance(col_dtype, PeriodDtype):\n",
    "                df[col] = df[col].dt.to_timestamp() if convert_period_to == 'timestamp' else df[col].astype(str)\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Handle object columns\n",
    "        if col_dtype == object:\n",
    "            sample = df[col].dropna().head(50)\n",
    "\n",
    "            # Check for Period objects\n",
    "            if not sample.empty and any(isinstance(x, pd.Period) for x in sample):\n",
    "                df[col] = df[col].apply(\n",
    "                    lambda p: p.to_timestamp() if isinstance(p, pd.Period) else p\n",
    "                ) if convert_period_to == 'timestamp' else df[col].astype(str)\n",
    "                continue\n",
    "\n",
    "            # Auto-detect datetime strings with warning suppression\n",
    "            str_sample = sample.astype(str).str.strip().head(20)\n",
    "            if len(str_sample) > 0:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "                    parsed_sample = pd.to_datetime(str_sample, errors='coerce', utc=True)\n",
    "\n",
    "                if parsed_sample.notna().sum() / max(1, len(str_sample)) > 0.4:\n",
    "                    try:\n",
    "                        with warnings.catch_warnings():\n",
    "                            warnings.filterwarnings('ignore', message='Could not infer format')\n",
    "                            df[col] = pd.to_datetime(df[col], errors='coerce', utc=True)\n",
    "                        continue\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load FinBERT model and tokenizer \n",
    "\n",
    "def load_finbert_model(model_name: str = MODEL_NAME):\n",
    "    \"\"\"Load FinBERT tokenizer and model.\"\"\"\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    logging.info(f\"Loading FinBERT model on {device}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model, device\n",
    "\n",
    "\n",
    "def predict_article_sentiment(tokenizer, model, device, title, text,\n",
    "                              chunk_chars=CHUNK_CHARS, max_chunks=MAX_ARTICLE_CHUNKS,\n",
    "                              batch_size=8, max_length=512):\n",
    "    \"\"\"\n",
    "    Predict sentiment for an article using FinBERT.\n",
    "    Returns dict with avg_probs, final_label, and n_pieces.\n",
    "    \"\"\"\n",
    "    # Build pieces (title + text chunks)\n",
    "    pieces = []\n",
    "    if title:\n",
    "        pieces.append(title)\n",
    "    if text:\n",
    "        chunks = chunk_text(text, chunk_chars)\n",
    "        pieces.extend(chunks)\n",
    "\n",
    "    if not pieces:\n",
    "        return None\n",
    "\n",
    "    pieces = pieces[:max_chunks]\n",
    "\n",
    "    # Batch process\n",
    "    all_probs = []\n",
    "    label_names = None\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(pieces), batch_size):\n",
    "            batch_texts = pieces[i:i+batch_size]\n",
    "            inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True,\n",
    "                             padding=True, max_length=max_length)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            # Get label names\n",
    "            if label_names is None:\n",
    "                try:\n",
    "                    id2lab = model.config.id2label\n",
    "                    label_names = [id2lab[i].lower() for i in range(len(id2lab))]\n",
    "                except Exception:\n",
    "                    label_names = ['positive', 'neutral', 'negative'] if probs.shape[1] == 3 else [f\"lab_{j}\" for j in range(probs.shape[1])]\n",
    "\n",
    "            # Store probabilities\n",
    "            for row in probs:\n",
    "                all_probs.append({label_names[j]: float(row[j]) for j in range(len(row))})\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Sentiment prediction failed: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "\n",
    "    # Aggregate: mean probability per label\n",
    "    agg = {}\n",
    "    for d in all_probs:\n",
    "        for k, v in d.items():\n",
    "            agg.setdefault(k, []).append(v)\n",
    "\n",
    "    avg_probs = {k: float(np.mean(vlist)) for k, vlist in agg.items()}\n",
    "\n",
    "    # Ensure all expected labels exist\n",
    "    for k in [\"positive\", \"neutral\", \"negative\"]:\n",
    "        avg_probs.setdefault(k, 0.0)\n",
    "\n",
    "    final_label = max(avg_probs.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "    return {\"avg_probs\": avg_probs, \"final_label\": final_label, \"n_pieces\": len(all_probs)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 18:10:17,034 INFO ============================================================\n",
      "2026-01-21 18:10:17,035 INFO STEP 1: Fetching GDELT article URLs\n",
      "2026-01-21 18:10:17,035 INFO ============================================================\n",
      "2026-01-21 18:10:17,036 INFO Querying GDELT for 2024-01-01 -> 2024-01-31\n",
      "2026-01-21 18:10:17,720 WARNING GDELT query failed: 429 -> Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "\n",
      "\n",
      "2026-01-21 18:10:17,723 WARNING Retrying GDELT query in 5s (attempt 1/3)\n",
      "2026-01-21 18:10:29,503 INFO Found 250 articles in this window\n",
      "2026-01-21 18:10:29,509 INFO Querying GDELT for 2024-02-01 -> 2024-02-29\n",
      "2026-01-21 18:10:37,722 INFO Found 250 articles in this window\n",
      "2026-01-21 18:10:37,730 INFO Querying GDELT for 2024-03-01 -> 2024-03-31\n",
      "2026-01-21 18:10:45,226 INFO Found 250 articles in this window\n",
      "2026-01-21 18:10:45,233 INFO Querying GDELT for 2024-04-01 -> 2024-04-30\n",
      "2026-01-21 18:11:03,959 INFO Found 250 articles in this window\n",
      "2026-01-21 18:11:03,961 INFO Querying GDELT for 2024-05-01 -> 2024-05-31\n",
      "2026-01-21 18:11:11,556 INFO Found 250 articles in this window\n",
      "2026-01-21 18:11:11,562 INFO Querying GDELT for 2024-06-01 -> 2024-06-30\n",
      "2026-01-21 18:11:22,467 INFO Found 250 articles in this window\n",
      "2026-01-21 18:11:22,474 INFO Querying GDELT for 2024-07-01 -> 2024-07-31\n",
      "2026-01-21 18:11:23,078 WARNING GDELT query failed: 429 -> Please limit requests to one every 5 seconds or contact kalev.leetaru5@gmail.com for larger queries.\n",
      "\n",
      "\n",
      "2026-01-21 18:11:23,082 WARNING Retrying GDELT query in 5s (attempt 1/3)\n",
      "2026-01-21 18:11:42,983 INFO Found 250 articles in this window\n",
      "2026-01-21 18:11:42,999 INFO Querying GDELT for 2024-08-01 -> 2024-08-31\n",
      "2026-01-21 18:11:52,115 INFO Found 250 articles in this window\n",
      "2026-01-21 18:11:52,123 INFO Querying GDELT for 2024-09-01 -> 2024-09-30\n",
      "2026-01-21 18:12:07,407 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:07,415 INFO Querying GDELT for 2024-10-01 -> 2024-10-31\n",
      "2026-01-21 18:12:18,693 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:18,701 INFO Querying GDELT for 2024-11-01 -> 2024-11-30\n",
      "2026-01-21 18:12:25,654 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:25,660 INFO Querying GDELT for 2024-12-01 -> 2024-12-31\n",
      "2026-01-21 18:12:32,409 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:32,416 INFO Querying GDELT for 2025-01-01 -> 2025-01-31\n",
      "2026-01-21 18:12:39,725 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:39,734 INFO Querying GDELT for 2025-02-01 -> 2025-02-28\n",
      "2026-01-21 18:12:58,324 INFO Found 250 articles in this window\n",
      "2026-01-21 18:12:58,332 INFO Querying GDELT for 2025-03-01 -> 2025-03-31\n",
      "2026-01-21 18:13:06,719 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:06,726 INFO Querying GDELT for 2025-04-01 -> 2025-04-30\n",
      "2026-01-21 18:13:13,885 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:13,892 INFO Querying GDELT for 2025-05-01 -> 2025-05-31\n",
      "2026-01-21 18:13:25,665 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:25,673 INFO Querying GDELT for 2025-06-01 -> 2025-06-30\n",
      "2026-01-21 18:13:33,202 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:33,211 INFO Querying GDELT for 2025-07-01 -> 2025-07-31\n",
      "2026-01-21 18:13:45,738 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:45,745 INFO Querying GDELT for 2025-08-01 -> 2025-08-31\n",
      "2026-01-21 18:13:58,127 INFO Found 250 articles in this window\n",
      "2026-01-21 18:13:58,135 INFO Querying GDELT for 2025-09-01 -> 2025-09-30\n",
      "2026-01-21 18:14:05,188 INFO Found 250 articles in this window\n",
      "2026-01-21 18:14:05,196 INFO Querying GDELT for 2025-10-01 -> 2025-10-31\n",
      "2026-01-21 18:14:12,448 INFO Found 250 articles in this window\n",
      "2026-01-21 18:14:12,452 INFO Querying GDELT for 2025-11-01 -> 2025-11-30\n",
      "2026-01-21 18:14:20,079 INFO Found 250 articles in this window\n",
      "2026-01-21 18:14:20,083 INFO Querying GDELT for 2025-12-01 -> 2025-12-31\n",
      "2026-01-21 18:14:29,863 INFO Found 250 articles in this window\n",
      "2026-01-21 18:14:29,871 INFO Querying GDELT for 2026-01-01 -> 2026-01-21\n",
      "2026-01-21 18:14:38,573 INFO Found 250 articles in this window\n",
      "2026-01-21 18:14:38,582 INFO GDELT URL fetching complete. Total unique URLs: 6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta saved to gdelt_query_urls.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: Fetch GDELT article URLs ===\n",
    "\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\"STEP 1: Fetching GDELT article URLs\")\n",
    "logging.info(\"=\" * 60)\n",
    "\n",
    "meta_file = fetch_all_gdelt_urls(QUERY, START_DATE, END_DATE, TEMP_META)\n",
    "print(f'Meta saved to {meta_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 18:20:33,957 INFO ============================================================\n",
      "2026-01-21 18:20:33,959 INFO STEP 2: Loading and sampling metadata\n",
      "2026-01-21 18:20:33,960 INFO ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique URLs: 6250\n"
     ]
    }
   ],
   "source": [
    "# === STEP 2: Load and sample metadata ===\n",
    "\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\"STEP 2: Loading and sampling metadata\")\n",
    "logging.info(\"=\" * 60)\n",
    "\n",
    "meta_rows = load_urls_from_meta(TEMP_META)\n",
    "meta_df = pd.DataFrame(meta_rows)\n",
    "meta_df['url'] = meta_df['url'].astype(str)\n",
    "meta_df.drop_duplicates(subset=['url'], inplace=True)\n",
    "print(f'Total unique URLs: {len(meta_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 250 articles (25 weeks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/0049fg855_5dzk88ykpjq8_r0000gn/T/ipykernel_99343/4093820290.py:7: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  meta_df['week'] = meta_df['seendate_parsed'].dt.to_period('W-MON').dt.to_timestamp()\n",
      "/var/folders/qk/0049fg855_5dzk88ykpjq8_r0000gn/T/ipykernel_99343/4093820290.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = meta_df.groupby('week').apply(\n"
     ]
    }
   ],
   "source": [
    "# Parse seendate and drop invalid entries \n",
    "\n",
    "meta_df['seendate_parsed'] = pd.to_datetime(meta_df['seendate'], errors='coerce', utc=True)\n",
    "meta_df = meta_df.dropna(subset=['seendate_parsed'])\n",
    "\n",
    "# FIXED: Use to_timestamp() to avoid PeriodIndex issues\n",
    "meta_df['week'] = meta_df['seendate_parsed'].dt.to_period('W-MON').dt.to_timestamp()\n",
    "\n",
    "# Sample articles per week\n",
    "sampled_df = meta_df.groupby('week').apply(\n",
    "    lambda g: g.sample(n=min(len(g), ARTICLES_PER_WEEK), random_state=RANDOM_STATE)\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"Selected {len(sampled_df)} articles ({sampled_df['week'].nunique()} weeks)\")\n",
    "meta_df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 18:21:38,708 INFO ============================================================\n",
      "2026-01-21 18:21:38,710 INFO STEP 3: Extracting text and analyzing sentiment\n",
      "2026-01-21 18:21:38,710 INFO ============================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract text and run FinBERT\n",
    "\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\"STEP 3: Extracting text and analyzing sentiment\")\n",
    "logging.info(\"=\" * 60)\n",
    "\n",
    "# Load existing results for resume\n",
    "done_df = pd.DataFrame()\n",
    "if os.path.exists(OUT_PARQUET):\n",
    "    logging.info(f'Loading existing parquet {OUT_PARQUET} for resume')\n",
    "    done_df = pd.read_parquet(OUT_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 18:22:06,276 INFO Loading FinBERT model on cpu\n"
     ]
    }
   ],
   "source": [
    "# Prepare set of processed URLs for quick lookup\n",
    "\n",
    "# FIXED: Use set for O(1) lookup instead of O(n)\n",
    "processed_urls = set(done_df['url'].values) if not done_df.empty else set()\n",
    "\n",
    "# Load FinBERT model\n",
    "tokenizer, model, device = load_finbert_model(MODEL_NAME)\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "for idx, r in tqdm(meta_df.iterrows(), total=len(meta_df), desc='Processing articles'):\n",
    "    url = r['url']\n",
    "\n",
    "    # FIXED: O(1) lookup\n",
    "    if url in processed_urls:\n",
    "        continue\n",
    "\n",
    "    title_meta = r.get('title') or ''\n",
    "    title, text, pubdate = extract_text_with_newspaper(url, fallback_text=title_meta)\n",
    "    time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n",
    "\n",
    "    if not title and not text:\n",
    "        logging.info(f'No content for url {url} (skipping)')\n",
    "        continue\n",
    "\n",
    "    pred = predict_article_sentiment(tokenizer, model, device, title or title_meta, text or '')\n",
    "    if pred is None:\n",
    "        continue\n",
    "\n",
    "    avg_probs = pred['avg_probs']\n",
    "    rec = {\n",
    "        'url': url,\n",
    "        'rss_title': title_meta,\n",
    "        'title': title,\n",
    "        'published_raw': pubdate if pubdate else r.get('seendate'),\n",
    "        'sent_label': pred['final_label'],\n",
    "        'prob_positive': avg_probs.get('positive', 0.0),\n",
    "        'prob_neutral': avg_probs.get('neutral', 0.0),\n",
    "        'prob_negative': avg_probs.get('negative', 0.0),\n",
    "        'n_pieces': pred['n_pieces']\n",
    "    }\n",
    "    new_rows.append(rec)\n",
    "    processed_urls.add(url)  # FIXED: Add to set\n",
    "\n",
    "    # Save every 50 articles\n",
    "    if len(new_rows) >= 50:\n",
    "        logging.info(f'Checkpoint: Saving {len(new_rows)} new rows ({len(done_df) + len(new_rows)} total)')\n",
    "        chunk_df = pd.DataFrame(new_rows)\n",
    "        done_df = pd.concat([done_df, chunk_df], ignore_index=True) if not done_df.empty else chunk_df.copy()\n",
    "        safe_df = sanitize_df_for_parquet(done_df, convert_period_to='timestamp')\n",
    "        safe_to_parquet(safe_df, OUT_PARQUET, index=False)\n",
    "        new_rows = []\n",
    "\n",
    "# Final save\n",
    "if new_rows:\n",
    "    logging.info(f'Final save: {len(new_rows)} new rows')\n",
    "    chunk_df = pd.DataFrame(new_rows)\n",
    "    done_df = pd.concat([done_df, chunk_df], ignore_index=True) if not done_df.empty else chunk_df.copy()\n",
    "    safe_df = sanitize_df_for_parquet(done_df, convert_period_to='timestamp')\n",
    "    safe_to_parquet(safe_df, OUT_PARQUET, index=False)\n",
    "\n",
    "logging.info(f'Finished extraction & prediction; total articles: {len(done_df)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate weekly sentiment plot\n",
    "logging.info(\"=\" * 60)\n",
    "logging.info(\"STEP 4: Generating weekly sentiment plot\")\n",
    "logging.info(\"=\" * 60)\n",
    "\n",
    "if 'done_df' not in locals() or done_df.empty:\n",
    "    if os.path.exists(OUT_PARQUET):\n",
    "        done_df = pd.read_parquet(OUT_PARQUET)\n",
    "    else:\n",
    "        raise FileNotFoundError('No prediction parquet found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define x-axis range\n",
    "start_date = pd.Timestamp(\"2023-10-20\", tz='UTC')\n",
    "end_date   = pd.Timestamp(\"2025-10-19\", tz='UTC')\n",
    "\n",
    "# Disable LaTeX rendering to suppress font warnings\n",
    "mpl.rcParams['text.usetex'] = False\n",
    "# Disable tokenizers parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the model output (reuse in-memory results if available)\n",
    "try:\n",
    "    df = done_df.copy()\n",
    "except NameError:\n",
    "    df = pd.read_parquet(OUT_PARQUET)\n",
    "\n",
    "# Ensure datetime and index\n",
    "df['published'] = pd.to_datetime(df['published_raw'], errors='coerce', utc=True)\n",
    "df = df.dropna(subset=['published'])\n",
    "df = df.set_index('published').sort_index()\n",
    "\n",
    "# Compute weekly sentiment averages (Monday-anchored)\n",
    "weekly = df[['prob_positive', 'prob_neutral', 'prob_negative']].resample('W-MON').mean().sort_index()\n",
    "\n",
    "btc = yf.download(\"BTC-USD\", start=\"2023-10-20\", end=\"2025-10-20\", progress=False)\n",
    "btc.index = pd.to_datetime(btc.index).tz_localize('UTC')\n",
    "\n",
    "# Resample to weekly closing price (Monday)\n",
    "btc_weekly = btc['Close'].resample('W-MON').mean()\n",
    "\n",
    "# Compute weekly change in price (Δ price = current week - previous week)\n",
    "btc_weekly_change = btc_weekly.diff()\n",
    "btc_weekly_change = btc_weekly.pct_change()  # percentage change\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Weekly bar width and offset\n",
    "bar_width = pd.Timedelta(days=2)\n",
    "offset = pd.Timedelta(days=2)\n",
    "x = weekly.index\n",
    "\n",
    "# Plot sentiment grouped bars\n",
    "ax.bar(x - offset, weekly['prob_positive'], width=bar_width, label='Positive', color='blue')\n",
    "ax.bar(x,          weekly['prob_neutral'],  width=bar_width, label='Neutral',  color='cyan')\n",
    "ax.bar(x + offset, weekly['prob_negative'], width=bar_width, label='Negative', color='red')\n",
    "\n",
    "# Left axis: sentiment\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Average sentiment probability')\n",
    "ax.set_title(f\"Weekly FinBERT Sentiment + BTC Weekly Fractional Price Change ({START_DATE.date()} → {END_DATE.date()})\")\n",
    "ax.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_xlim([start_date, end_date])\n",
    "\n",
    "# X-axis formatting\n",
    "locator = mdates.AutoDateLocator()\n",
    "formatter = mdates.ConciseDateFormatter(locator)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(btc_weekly_change.index, btc_weekly_change, label='BTC Weekly Fractional Price Change (USD)', color='orange', linewidth=1, ls='--')\n",
    "ax2.axhline(0, color='gray', linestyle='--', linewidth=1)  # zero-line\n",
    "ax2.set_ylabel('BTC Weekly Change (USD)', color='black')\n",
    "ax2.tick_params(axis='y', labelcolor='black')\n",
    "\n",
    "# Combine legends\n",
    "lines_1, labels_1 = ax.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(OUT_WEEKLY_PNG.replace('.png', '_btc_change_overlay.png'), dpi=150)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
